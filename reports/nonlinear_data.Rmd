---
title: "Non-linearity in Reaction Time Data"
author: "Nicolas Restrepo"
date: "2/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, fig.align='center')
```

## Are non linear models more appropriate here?

```{r}
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggrepel)
library(gridExtra)
library(haven)
library(lavaan)
library(lme4)
library(nlme)
library(stargazer)
library(broom)
library(psych)
library(brms)
library(plotly)
library(sjPlot)
library(corrplot)
library(ggdag)
library(quanteda)
theme_set(theme_light())
```

Here, I am going to explore whether the relationship between distance from the prototype and reaction could be better expressed non-linearly. Let me begin by importing the data and reshaping it. 

```{r, include=FALSE}

# Load in conservative data
rt_conservative <- read_csv("rt_conservative_February 10, 2020_10.42.csv")

# Load in liberal data 
rt_liberal <- read_csv("rt_liberal.csv")

# Delete unnecessary rows 
rt_conservative <- rt_conservative[-(1:2),]
rt_liberal <- rt_liberal[-(1:2),]

# How many people passed the attention checks?
rt_conservative <- rt_conservative %>% 
  mutate(passed = if_else(att_1 == "Harmless (K)" & att_2 == "Immoral (J)" & att_3 == "Harmful (J)", 1, 0)) 

rt_conservative %>% 
  group_by(passed) %>% 
  summarise(n())

rt_liberal <- rt_liberal %>% 
  mutate(passed = if_else(att_1 == "Harmless (K)" & att_2 == "Immoral (J)" & att_3 == "Harmful (J)", 1, 0)) 

rt_liberal %>% 
  group_by(passed) %>% 
  summarise(n()) 

# Join both datasets 
data <- rbind(rt_liberal, rt_conservative)

# Filter people who did not pass
data <- data %>% 
  filter(passed == 1)

# Now let's turn this into a long dataframe 

# Create a manageable wide dataframe 

rtw <- data %>% 
  select(ends_with("Page Submit"), i_phc, i_pkp, i_tmdp, i_adr, i_elb, 
         i_acr, i_jbd, i_scc, i_ddf, i_git, i_idb, i_ayc, i_sip, i_ecc, 
         i_ccr, i_mbr, i_msn, i_mbw, i_php, i_mmc, i_mmls, i_msas, i_mpsa, i_whh, i_thls, h_phc, h_pkp, h_tmdp, h_adr, h_elb, 
         h_acr, h_jbd, h_scc, h_ddf, h_git, h_idb, h_ayc, h_sip, h_ecc, 
         h_ccr, h_mbr, h_msn, h_mbw, h_php, h_mmc, h_mmls, h_msas, h_mpsa, h_whh, h_thls) %>% 
  mutate(ids = 1:184) %>% 
  select(ids, everything())


# Create a long dataframe for harm and reaction time

d_long_harm_rt <- rtw %>% 
  select(ids, starts_with("h_")) %>% 
  pivot_longer(2:26, names_to = "scenario", values_to = "reaction_time_harm") %>% 
  select(ids, scenario, reaction_time_harm) %>% 
  mutate(scenario = str_remove(scenario, "_t_Page Submit$")) %>% 
  mutate(scenario = str_remove(scenario, "^h_"))

# Create a long dataframe for is_harm

d_long_harm_bin <- rtw %>% 
  select(ids, starts_with("h_")) %>% 
  pivot_longer(27:51, names_to = "scenario", values_to = "is_harmful") %>% 
  select(ids, scenario, is_harmful) %>% 
  mutate(scenario = str_remove(scenario, "^h_"))

d_long_harm <- left_join(d_long_harm_rt, d_long_harm_bin, by = c('ids', "scenario"))

# Create a long dataframe for immorality and reaction time 

d_long_imm_rt <- rtw %>% 
  select(ids, starts_with("i_")) %>% 
  pivot_longer(2:26, names_to = "scenario", values_to = "reaction_time_imm") %>% 
  select(ids, scenario, reaction_time_imm) %>% 
  mutate(scenario = str_remove(scenario, "_t_Page Submit$")) %>% 
  mutate(scenario = str_remove(scenario, "^i_"))

# Create a long dataframe for immorality and is_immoral

d_long_imm_bin <- rtw %>% 
  select(ids, starts_with("i_")) %>% 
  pivot_longer(27:51, names_to = "scenario", values_to = "is_immoral") %>% 
  select(ids, scenario, is_immoral) %>% 
  mutate(scenario = str_remove(scenario, "^i_"))

d_long_imm <- left_join(d_long_imm_rt, d_long_imm_bin, by = c('ids', 'scenario'))

# Now join them 

d_long <- left_join(d_long_harm, d_long_imm, by = c('ids', 'scenario')) %>% 
  mutate_at(c('reaction_time_harm', 'reaction_time_imm'), as.numeric)


# Let's get some information about the events in there 

# Load dataset with the deflections 

selected_events <- read_csv("data/full_events.csv")

# Create column for abreviations 

events <- c("phc", 
            "pkp", 
            "tmdp", 
            "adr", 
            "elb", 
            "acr", 
            "jbd", 
            "scc", 
            "ddf", 
            "git", 
            "idb", 
            "ayc", 
            "sip", 
            "ecc", 
            "ccr", 
            "mbr", 
            "msn", 
            "mbw", 
            "php", 
            "mmc", 
            "mmls", 
            "msas", 
            "mpsa", 
            "whh", 
            "thls")

# Create new column 

events_def <- selected_events %>% 
  mutate(scenario = events) %>% 
  select(scenario, def, type, 5:33)

# Add to long dataset 

d_long <- d_long %>% 
  left_join(events_def, by = "scenario")

# How many weird responses do we have? 

sum(d_long$reaction_time_harm < 1)
sum(d_long$reaction_time_imm < 1)
sum(d_long$reaction_time_harm > 10)
sum(d_long$reaction_time_imm > 10)

# There are 66 responses that don't seem valid. Let's replace them with the median imputation 
# First I need to calculate the median for the scenarios 

median_rt <- d_long %>% 
  group_by(scenario) %>% 
  summarise(med_imm = median(reaction_time_imm), 
            med_harm = median(reaction_time_harm))

# Function for replacing immorality values 

replace_imm <- function(x) {
  
ss <- d_long[x, 2] %>% 
  as.vector() %>% 
  as.character()

i_scenario <- median_rt %>% 
  filter(str_detect(scenario, ss)) %>% 
  select(med_imm) %>% 
  as.numeric() %>% 
  as.vector()

d_long[x, 5] <<- i_scenario

}

# Function for replacing harm values 

replace_harm <- function(x) {
  
ss <- d_long[x, 2] %>% 
  as.vector() %>% 
  as.character()

h_scenario <- median_rt %>% 
  filter(str_detect(scenario, ss)) %>% 
  select(med_harm) %>% 
  as.numeric() %>% 
  as.vector()

d_long[x, 3] <<- h_scenario

}

# Map all small immorality values 
smaller_imm <- which(d_long$reaction_time_imm < 1)

map(.x = smaller_imm, .f = replace_imm)

# Map all small harm values 
smaller_harm <- which(d_long$reaction_time_harm < 1)

map(.x = smaller_harm, .f = replace_harm)

# Map all big immorality values 
large_imm <- which(d_long$reaction_time_imm > 10)

map(.x = large_imm, .f = replace_imm)

# Map all large harm values 
large_harm <- which(d_long$reaction_time_harm > 10)

map(.x = large_harm, .f = replace_harm)

# Recode variables to binary 

d_long <- d_long %>%  
  mutate(is_immoral = case_when(is_immoral == "Immoral (J)" ~ 1, 
                                is_immoral == "Not Immoral (K)" ~ 0), 
         is_harmful = case_when(is_harmful == "Harmful (J)" ~ 1, 
                                is_harmful == "Harmless (K)" ~ 0))

# Create a new variable for the variances 

d_long_var <- d_long %>% 
  group_by(scenario) %>% 
  summarise(p_harm = mean(is_harmful, na.rm = T), 
            p_imm = mean(is_immoral, na.rm = T), 
            v_harm = var(is_harmful, na.rm = T), 
            v_imm = var(is_immoral, na.rm = T))

# Join back to the long dataframe 

d_long <- left_join(d_long, d_long_var, by = "scenario")

dsl <- d_long %>% 
  group_by(scenario, type) %>% 
  summarise(mean_harm = mean(reaction_time_harm), 
            mean_imm = mean(reaction_time_imm)) %>% 
  ungroup()

# Name the scenarios with the full length 

dsl <- dsl %>% 
  mutate(full_string = case_when(scenario == "acr" ~ "An athlete cheats their rival", 
                                 scenario == "adr" ~ "An athlete deceives the referee", 
                                 scenario == "ayc" ~ "An athlete yells at their coach", 
                                 scenario == "ccr" ~ "A coach cheers for the rival", 
                                 scenario == "ddf" ~ "A daughter disobeys her father", 
                                 scenario == "ecc" ~ "An employee conspires with a competitor", 
                                 scenario == "elb" ~ "An employee lies to the boss", 
                                 scenario == "git" ~ "A girl interrupts her teacher", 
                                 scenario == "idb" ~ "An intern disobeys their boss", 
                                 scenario == "jbd" ~ "A judge befriends the defendant", 
                                 scenario == "mbr" ~ "A man betrays his relative", 
                                 scenario == "mbw" ~ "A man betrays his wife", 
                                 scenario == "mmc" ~ "A man marries his cousin", 
                                 scenario == "mmls" ~ "A man makes love to his sister", 
                                 scenario == "mpsa" ~ "A married person has sex with an adulterer", 
                                 scenario == "msas" ~ "A mother sexually arouses her son", 
                                 scenario == "msn" ~ "A mayor slanders a neighbor", 
                                 scenario == "phc" ~ "A person hurts a child", 
                                 scenario == "php" ~ "A person hires a prostitute", 
                                 scenario == "pkp" ~ "A person kills a person", 
                                 scenario == "scc" ~ "A student cheats their classmate", 
                                 scenario == "sip" ~ "A student insults the professor", 
                                 scenario == "thls" ~ "A teacher hits a lazy student", 
                                 scenario == "whh" ~ "A wife hits her forgetful husband", 
                                 scenario == "tmdp" ~ "A teenager mocks a disabled person"))

# Add the short versions again for the plot

dsl <- dsl %>% 
  mutate(length = str_count(full_string)) 

# Calculate readibility 

readibility_indices <- c(NA, rep = 25)

for (i in 1: 25) {
  
  y <- textstat_readability(dsl$full_string[i], measure = "Flesch.Kincaid")
  readibility_indices[i] <- y$Flesch.Kincaid
}

# Join with dsl 

dsl <- cbind(dsl, readibility_indices)

# Create function to calculate the euclidean distance in three-dimensional space 

events_values <- events_def %>% 
  select(scenario, op, be, bp)

eucl_dist_pkp <- function(x) { 
    op <-  events_values[x, 2] 
    be <-  events_values[x, 3] 
    bp <-  events_values[x, 4]
  
   # Formula   
  # The values of the prototype are always the ones the events are compared against
  distance <- sqrt((0.95 - op)^2 + ((-4.26) - be)^2 + (1.95 - bp)^2)
   return(as.double(distance))
}


distances_pkp <- map_dbl(1:25, eucl_dist_pkp)

events_values_pkp <- events_values %>% 
  cbind(distances_pkp) %>% 
  select(scenario, distances_pkp)

# Join values with the long dataset 

dsl <- dsl %>% 
  left_join(events_values_pkp, by = "scenario")

# Create new function with Person hurts child as the prototype 

eucl_dist_phc <- function(x) { 
    op <-  events_values[x, 2] 
    be <-  events_values[x, 3] 
    bp <-  events_values[x, 4]
  
   # Formula   
  # The values of the prototype are always the ones the events are compared against
  distance <- sqrt(((-1.14) - op)^2 + ((-3.17) - be)^2 + (1.06 - bp)^2)
   return(as.double(distance))
}


distances_phc <- map_dbl(1:25, eucl_dist_phc)

events_values_phc <- events_values %>% 
  cbind(distances_phc) %>% 
  select(scenario, distances_phc)

# Join values with the long dataset 

dsl <- dsl %>% 
  left_join(events_values_phc, by = "scenario")

# Create new function with Person hurts child as the prototype 

eucl_dist_prot <- function(x) { 
  op <-  events_values[x, 2] 
  be <-  events_values[x, 3] 
  bp <-  events_values[x, 4]
  
  # Formula   
  # The values of the prototype are always the ones the events are compared against
  distance <- sqrt(((-1.14) - op)^2 + ((-4.26) - be)^2 + (1.95 - bp)^2)
  return(as.double(distance))
}


distances_prot <- map_dbl(1:25, eucl_dist_prot)

events_values_prot <- events_values %>% 
  cbind(distances_prot) %>% 
  select(scenario, distances_prot)

# Join values with the long dataset 

dsl <- dsl %>% 
  left_join(events_values_prot, by = "scenario")

lengths <- dsl %>% 
  select(scenario, length, readibility_indices, distances_phc, distances_pkp, distances_prot)

d_long <- d_long %>% 
  select(1:16, 36:39) %>% 
  left_join(lengths, by = "scenario")

# Add one column for the average reaction time of the person 

# Create the averages 
avg_rt <- d_long %>% 
  select(ids, reaction_time_harm, reaction_time_imm) %>% 
  pivot_longer(2:3) %>% 
  group_by(ids) %>% 
  summarise(avg_rt = mean(value))

# Add averages to long data 

d_long <- left_join(d_long, avg_rt, by = "ids")

# Add ideological beliefs

data_pol <- data %>% 
  select(ideology_1) %>% 
  mutate(ids = 1:184)

d_long <- left_join(d_long, data_pol, by = "ids")



```

Let's begin by thinking about the expected relationships between the variables. We have two pieces of information that should be informative when predicting reaction time: distance from the prototype and ambiguity. The latter should be conceived as how much agreement there was amongst respondents that an event was either immoral or harmful. We can envision two measures for this: either the proportion of events that were classified as either harmful or immoral or the variance of the same measure. Let's explore these correlation between these variables. 

```{r}

d_long %>% 
  select(reaction_time_harm, reaction_time_imm, p_harm, p_imm, v_harm, v_imm, distances_prot) %>% 
  cor() %>% 
  corrplot(method = "number", sig.level = T)
```

The correlations look as we expected. As distance from the prototype increases, the proportions of harm and immorality decrease. Expectedly, the opposite (but mirroring) relationship occurs for the variances of harm and immorality: more ambiguous scenarios tend to be further away from the prototype. The relationships between these variables and reaction time, however, seem rather week. It might be perhaps that these relationships are non-linear. 

Let's think about non-linearity in terms of distance from the prototype and reaction time. If we posit that this relationship is linear, then implicitly argue that every unit of distance away from the template should result in a commesurate increase in reaction time. Instead, we can think about fuzzy categories as gravitational fields. Initially, distance from the template should be positively related with cognitive effort. However, there must be a tipping point after which events start seemingly obviously not immoral; the point at which you have left the gravitational pull of the category. The relationship, in this case, would be described as a parabola. Additionally, we could think about this relationship logarithmically. Initial moves away from the prototype should have a big impact on cognitive effort. As distance increases, though, the effect of moving further away becomes increasingly smaller. 

Similarly, we can envision similar non-linear relationships between reaction time and "ambiguity" - i.e. proportion of respondents who said an event was either harmful or immoral. We can imagine for instance that people categorize quickly unambiguous events, whether they are immoral or not. This would, then, also be expressed by a quadratic expression.

We can test these models but before it would be useful to visualize the data to see whether these relationships are indeed plausible. 

```{r}

p1 <- d_long %>% 
  ggplot(aes(y = reaction_time_imm, x = distances_prot)) + 
  geom_jitter(width = 0.1, height = 0.1, alpha = 0.2) + 
  geom_smooth(method = "loess") + 
  labs(x = "Distance from Prototype", 
       y = "RT Immorality")

p2 <- d_long %>% 
  ggplot(aes(y = reaction_time_harm, x = distances_prot)) + 
  geom_jitter(width = 0.1, height = 0.1, alpha = 0.2) + 
  geom_smooth(method = "loess") + 
  labs(x = "Distance from Prototype", 
       y = "RT Harm")

p3 <- d_long %>% 
  ggplot(aes(y = reaction_time_harm, x = p_harm)) + 
  geom_jitter(width = 0.1, height = 0.1, alpha = 0.2) + 
  geom_smooth(method = "loess") + 
  labs(x = "Proportion Harmfulness", 
       y = "RT Harm")

p4 <- d_long %>% 
  ggplot(aes(y = reaction_time_imm, x = p_imm)) + 
  geom_jitter(width = 0.1, height = 0.1, alpha = 0.2) + 
  geom_smooth(method = "loess") + 
  labs(x = "Proportion Immoral", 
       y = "RT Immoral")

grid.arrange(p1,p2,p3,p4)
```

The data show that non-linearity is certainly plausible. Now, I am going to test which model best reflects the relationship between distance from the prototype and reaction time. 

```{r}

m1 <- lmer(reaction_time_harm ~ 1 + distances_prot + length + (1 | ids) + (1 | scenario),
           data = d_long)
tab_model(m1)
m2 <- lmer(reaction_time_harm ~ 1 + distances_prot + I(distances_prot^2) + length + readibility_indices + (1 | ids) + (1 | scenario),
           data = d_long)
tab_model(m2)
m3 <- lmer(reaction_time_harm ~ 1 + log(distances_prot) + length + (1 | ids) + (1 | scenario),
           data = d_long)
tab_model(m3)
BIC(m1,m2,m3)
```

If we compare the measures of fit, the model that best captures the relationship in the data is the linear-log model. We can plot these relationship to better understand it. 

```{r}

plot_model(m3, type = "pred", terms = "distances_prot")
```

We see that the non-linear relationship looks as expected. Initial moves away from the template are "costly" but then the impact of distance plateaus. This certainly reflects current understandings of how mental templates aid categorization. 

Now, let's look at the relationship between ambiguity and reaction time, comparing the different ways of expressing this relationship. 

```{r}
m4 <- lmer(reaction_time_harm ~ 1 + p_harm + length + (1 | ids) + (1 | scenario),
           data = d_long)
tab_model(m4)
m5 <- lmer(reaction_time_harm ~ 1 + p_harm + I(p_harm^2) + length + (1 | ids) + (1 | scenario),
           data = d_long)
tab_model(m5)
m6 <- lmer(reaction_time_harm ~ 1 + log(p_harm) + length + (1 | ids) + (1 | scenario),
           data = d_long)
tab_model(m6)
BIC(m4,m5,m6)

```

Here the relationship is most appropriately described as quadratic. Respondents categorize unambiguous events more quickly, at either side of the moral spectrum. 

```{r}

plot_model(m5, type = "pred", terms = "p_harm [all]")
```

## What is the relationship between ambiguity and distance from the prototype? 

My first instinct was to put all these variables into the model predicting reaction time. Nonetheless, I wonder whether this is fundamentally at odds with the mechanism we are positing here. Ultimately, the argument we are trying to test is whether the attribution of immorality occurs is explained by distance from a cognitive template. In this model, then, the ambiguity of an event must be the result of its distance to a prototype; after all this is the cognitive presupposition we are buying into. "Controlling" for the proportion of respondents who found an event immoral would imply a causal model in which we think that ambiguity affects both distance from the prototype and reaction time: 

```{r}

tidy_ggdag <- dagify(
  RT ~ D,
  RT ~~ A, 
  A ~~ D,# bidirected path
  exposure = "D",
  outcome = "RT"
) %>% 
  tidy_dagitty()

ggdag(tidy_ggdag) +
  theme_dag()
```

This is, however, not consistent with our model of moral cognition. If prototypical categorization is the underlying mechanism then ambiguity is a necessarily an effect of distance itself. The model would look more like this: 

```{r}

tidy_ggdag2 <- dagify(
  RT ~ D,
  RT ~ A, 
  A ~ D,# bidirected path
  exposure = "D",
  outcome = "RT"
) %>% 
  tidy_dagitty()

ggdag(tidy_ggdag2) +
  theme_dag()
```

This diagram fits better with our thinking thorughout this project. One question we could ask then is whether distance affects reaction time mainly through ambiguity. But I am not sure whether that statement is purely circular. 