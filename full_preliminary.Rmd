---
title: "Preliminary Analysis"
author: "Nicolas Restrepo"
date: "11/15/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

## Data Preparation

```{r packages}
# Packages 
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggrepel)
library(gridExtra)
library(haven)
library(lavaan)
library(lme4)
library(nlme)
library(stargazer)
library(broom)
library(psych)
library(brms)
theme_set(theme_light())
```

I will begin by importing the data and getting it ready for analysis. Given that that the surveys were fielded at different points, there is a fair amount of merging we need to do. 

```{r}

# Load in the conservative data
c_data <- read_csv("data/conservatives_full.csv")

#Get rid of the first 2 rows 

c_data <- c_data[-(1:2), ]

# Load pretest data 

p_data <- read_csv("data/pretest.csv")

p_data <- p_data[-(1:3), ]

# Load the liberal data 

l_data <- read_csv("data/liberals_complete.csv")

l_data <- l_data[-(1:2),]

# Now bind them together 

data <- rbind(c_data, p_data, l_data)

```

Having merged the data I will filter the respondents who failed the attention checks. I will also create a variable to distinguish between liberals and conservatives. 

```{r}
# Get attention checks for the data 

data <- data %>% 
  mutate(passed_1 = ifelse(ach_1_1 == "extremely harmful", 1, 0), 
         passed_2 = ifelse(ach_2_1 == "not at all immoral", 1, 0), 
         passed_3 = ifelse(ach_3_1 == "moderately unexpected", 1, 0), 
         passed_4 = ifelse(ach_4_1 == "not at all harmful", 1, 0), 
         passed_5 = ifelse(ach_5_1 == "extremely immoral", 1, 0), 
         passed_6 = ifelse(ach_6_1 == "extremely unexpected", 1, 0), 
         sum_pass = passed_1 + passed_2 + passed_3 + passed_4 + passed_5 + passed_6)

# Now drop cases which did not pass the attention checks 

data <- data %>% 
  filter(sum_pass == 6)

# Create variable that indicates if someone is conservative

data <- data %>% 
  mutate(conservative = ifelse(ideology_1 == 5 | ideology_1 == 6 | ideology_1 == 7, 1, 0)) 

data %>% 
  group_by(conservative) %>% 
  summarise(n())

```

In total, we have 194 usable cases (99 liberals and 95 conservatives).

I am also going to recode the data so we get numbers instead of strings. 

```{r}
# Mutate all main questions so that we get numbers 

data <- data %>% 
  mutate_at(vars(contains("_h_1")), funs(recode(., 
                                   "not at all harmful" = 1, 
                                   "slightly harmful" = 2, 
                                   "moderately harmful" = 3, 
                                   "very harmful" = 4, 
                                   "extremely harmful" = 5))) %>% 
  mutate_at(vars(contains("_i_1")), funs(recode(., 
                                   "not at all immoral" = 1, 
                                   "slightly immoral" = 2, 
                                   "moderately immoral" = 3, 
                                   "very immoral" = 4, 
                                   "extremely immoral" = 5))) %>% 
  mutate_at(vars(contains("_u_1")), funs(recode(., 
                                   "not at all unexpected" = 1, 
                                   "slightly unexpected" = 2, 
                                   "moderately unexpected" = 3, 
                                   "very unexpected" = 4, 
                                   "extremely unexpected" = 5)))

```

For now, I am going to select only the ratings for each event and the respondent's political ideology:

```{r}

# Get of of the first set of uninformative columns 

d <- data %>% 
  select(-contains("ach")) %>% 
  select(pkp_h_1:msas_u_1, ideology_1, conservative)

# Now let's create a more amenable dataset 

d <- d %>% 
  mutate(id = 1:n()) %>% 
  select(id, everything()) 

# Rename all variables 

d <- d %>%  
  rename_all(
    funs(str_remove(., "_1"))
    ) %>% 
  mutate_if(is.character, as.numeric)

```

Now we have a manageable wide dataframe. However, at this point, it would also be useful to put the data into a long format. This will help us conduct multilevel analysis and make certain visualizations. 

```{r}

# Reshape harmful perceptions from wide to long 

d_harm_long <- d %>% 
  select(id, ideology, conservative, contains("_h")) %>% 
  gather(key = "scenario", value = "harm", 4:28) %>% 
  mutate(scenario = str_remove(scenario, "_h$"))

# Reshape immoral perceptions from wide to long 

d_imm_long <- d %>% 
  select(id, ideology, conservative, contains("_i")) %>% 
  gather(key = "scenario", value = "immoral", 4:28) %>% 
  mutate(scenario = str_remove(scenario, "_i$"))

# Reshape unexpected perceptions from wide to long 

d_un_long <- d %>% 
select(id, ideology, conservative, contains("_u")) %>% 
  gather(key = "scenario", value = "unexpected", 4:28) %>% 
  mutate(scenario = str_remove(scenario, "_u$"))

# Now join them 

d_long <- d_harm_long %>% 
  mutate(immoral = d_imm_long$immoral, 
         unexpected = d_un_long$unexpected)
```

Now I am going to create a column for each moral foundation and I will also add the information for each event. 

```{r}

# Load dataset with the deflections 

selected_events <- read_csv("data/full_events.csv")

# Create column for abreviations 

events <- c("pch", 
            "pkp", 
            "tmdp", 
            "adr", 
            "elb", 
            "acr", 
            "jbd", 
            "scc", 
            "ddf", 
            "git", 
            "idb", 
            "ayc", 
            "sip", 
            "ecc", 
            "ccr", 
            "mbr", 
            "msn", 
            "mbw", 
            "php", 
            "mmc", 
            "mmls", 
            "msas", 
            "mpsa", 
            "whh", 
            "thls")

# Create new column 

events_def <- selected_events %>% 
  mutate(scenario = events) %>% 
  select(scenario, def, type, 5:33)

# Add to long dataset 

d_long <- d_long %>% 
  left_join(events_def, by = "scenario")

# Create a variable for negative behavior 

d_long <- d_long %>% 
  mutate(neg_beh = ifelse(be < 0, 1, 0))

```

Alright, now we are done with the data preparation. We can move on to some descriptive analysis. 


## Descriptive Analysis 

I am going to make some descriptive plots for the average scores for each question on all three dimensions. To do this I first need to create a summary table. 

```{r}

# create a summary table for the values of each question 

gd <- d %>% 
  select(-c(id, ideology, conservative)) %>% 
  gather(key = "scenario", 
         value = "score") %>% 
  group_by(scenario) %>% 
  summarise_all(funs(med = median(.), avg = mean(.), maximum = max(.), minimum = min(.), st_dv = sd(.), fq = quantile(., 0.25), tq = quantile(., 0.75))) 

gd <- gd %>% 
  mutate(scenario = str_replace(scenario, "acr", "athlete_cheats_rival"), 
         scenario = str_replace(scenario, "adr", "athlete_deceives_referee"), 
         scenario = str_replace(scenario, "ayc", "athlete_yells_at_coach"), 
         scenario = str_replace(scenario, "ccr", "coach_cheers_rival"), 
         scenario = str_replace(scenario, "ddf", "daughter_disobeys_father"),
         scenario = str_replace(scenario, "ecc", "employee_conspires_with_comp."),
         scenario = str_replace(scenario, "elb", "employee_lies_to_boss"), 
         scenario = str_replace(scenario, "git", "girl_interrupts_teacher"),
         scenario = str_replace(scenario, "idb", "intern_disobeys_boss"), 
         scenario = str_replace(scenario, "jbd", "judge_befriends_defendant"), 
         scenario = str_replace(scenario, "mbr", "man_betrays_relative"), 
         scenario = str_replace(scenario, "mbw", "man_betrays_wife"), 
         scenario = str_replace(scenario, "mmc", "man_marries_cousin"),
         scenario = str_replace(scenario, "mmls", "man_makes_love_sister"), 
         scenario = str_replace(scenario, "mpsa", "married_has_sex_adulterer"), 
         scenario = str_replace(scenario, "msas", "mother_sexually_arouses_son"), 
         scenario = str_replace(scenario, "msn", "mayor_slanders_neighbor"), 
         scenario = str_replace(scenario, "pch", "person_hurts_child"), 
         scenario = str_replace(scenario, "php", "person_hires_prostitute"), 
         scenario = str_replace(scenario, "pkp", "person_kills_person"),
         scenario = str_replace(scenario, "scc", "student_cheats_classmate"), 
         scenario = str_replace(scenario, "sip", "student_insults_professor"), 
         scenario = str_replace(scenario, "thls", "teacher_hits_lazy_student"), 
         scenario = str_replace(scenario, "whh", "wife_hits_husband"), 
         scenario = str_replace(scenario, "tmdp", "teenager_mocks_disabled"))
```


Now that we have the table, we can produce the plots. 

```{r}

# Keep only harm questions 

gdh <- gd[str_which(gd$scenario, "_h$"),]

# Plot harm questions 

gdh %>% 
  mutate(scenario = str_remove(scenario, "_h$")) %>% 
  ggplot(aes(x = reorder(scenario, avg), y = avg)) + 
  geom_point(col = "darkred", fill = "darkred", pch = 2) + 
  geom_point(aes(y = med), col = "darkgreen", fill = "darkgreen") +
  geom_errorbar(aes(ymin = fq, ymax = tq), alpha = .45) +
  coord_flip() + 
  labs(x = "", 
       y = "Ratings", 
       title = "Perceptions of Harmfulness", 
       caption = "Circles = Median \nTriangles = Mean")

# Now lets look at immorality 

gdi <- gd[str_which(gd$scenario, "_i$"),]

# Plot immorality

gdi %>% 
  mutate(scenario = str_remove(scenario, "_i$")) %>% 
  ggplot(aes(x = reorder(scenario, avg), y = avg)) + 
  geom_point(col = "darkred", fill = "darkred", pch = 2) + 
  geom_point(aes(y = med), col = "darkgreen", fill = "darkgreen") +
  geom_errorbar(aes(ymin = fq, ymax = tq), alpha = .45) +
  coord_flip() + 
  labs(x = "", 
       y = "Ratings", 
       title = "Perceptions of Immorality", 
       caption = "Circles = Median \nTriangles = Mean")

# Lastly let's look at unexpectedness 

gdu <- gd[str_which(gd$scenario, "_u$"),]

# Plot Unexpectedness

gdu %>% 
  mutate(scenario = str_remove(scenario, "_u$")) %>% 
  ggplot(aes(x = reorder(scenario, avg), y = avg)) + 
  geom_point(col = "darkred", fill = "darkred", pch = 2) + 
  geom_point(aes(y = med), col = "darkgreen", fill = "darkgreen") +
  geom_errorbar(aes(ymin = fq, ymax = tq), alpha = .45) +
  coord_flip() + 
  labs(x = "", 
       y = "Ratings", 
       title = "Perceptions of Unexpectedness", 
       caption = "Circles = Median \nTriangles = Mean") 

```

The data show similar patterns to the ones we had identified before. A person killing a person is consistently considered the most immoral and the most harmful scenario. A mother sexually arousing her son is, in turn, considered to be the most unexpected event. Again, there are interesting patterns of agreement and disagreement. The middle 50% of the sample said that killing and mother/son incest are extremely immoral. The IQR is equally narrow for the perceptions of harmfulness of killing. This tells us that the respondents interpreted the scenarios consistently. Interestingly, there is considerable disgreement about the harmfulness of a man marrying his cousin, a man making love to his sister, and a man procuring a prostitute. I genuinely think this is a very interesting insight: it suggests that the debates around the wrongness of purity transgressions might be related to the fact that we differ in the extent to which we consider these acts harmful.

We can look at whether the harmfulness ratings for purity trangressions differ between conservatives and liberals. 

```{r}

d_long %>% 
  filter(type == "purity") %>% 
  ggplot(aes(x = reorder(scenario, harm), y = harm)) + 
  geom_boxplot(width = 0.3, fill = "white") +
  coord_flip() +
  facet_wrap(~ conservative) + 
  labs(x = "", 
       y = "Harmfulness", 
       title = "Harmfulness of Purity Trangressions", 
       subtitle = "Liberals and Conservatives")

```

Here, conservatives are on the right and liberals on the left. Even though they seem to agree on the ranking of the harmfulness of purity trangressions, liberals seem overall more permissive. This is particularly true for adultery and prostitution. 

The data also allow us to recreate the plot from Gray and Keeney (2015), where they visualize the average severity of events and their "weirdness". In the next plot, then, I will put immorality in the x-axis and unexpectedness in the y-axis. 

```{r}

# create mean measures from the long dataset

dsl <- d_long %>% 
  group_by(scenario, type, def, dop) %>% 
  summarise(mean_harm = mean(harm), 
            mean_imm = mean(immoral), 
            mean_unex = mean(unexpected)) 


# Recreate Gray and Keeney's plot

dsl %>% 
  ggplot(aes(x = mean_imm, y = mean_unex, col = type)) + 
  geom_point() + 
  geom_text_repel(aes(label = scenario), show.legend = F) + 
  theme(legend.position = "bottom") + 
  guides(text = F) + 
  labs(x = "Immorality", 
       y = "Unexpectedness", 
       title = "Average Unexpectedness by Average Immorality")
```

Some insights from Gray and Keeney (2015) still hold in our data. Certain purity trangressions are the most unexpected, but harmful wrongs are still the most severe. However, the plot suggests that the relationship between unexpectedness and severity is perhaps more linear than these authors have been willing to concede. They do argue that "weirdness" is related to morality insofar as it denotes norm violation. However, given the limited amount of scenarios they analyze, they do not have enough data to make this case convincingly. This relationship, nonetheless, is evident in our data. 

Let's look at the relationship between harm and unexpectedness, and harm and immorality. 

```{r}

# Plot harm by unexpectedness 

dsl %>% 
  ggplot(aes(x = mean_harm, y = mean_unex, col = type)) + 
  geom_point() + 
  geom_text_repel(aes(label = scenario), show.legend = F) + 
  theme(legend.position = "bottom") + 
  guides(text = F) + 
  labs(x = "Harmfulness", 
       y = "Unexpectedness", 
       title = "Average Unexpectedness by Average Harmfulness")

```

The relationship here is similar to the one above. Unsurprisingly, the most unexpected scenarios are those involving purity and the most harmful are those involving harm. However, affirming Kurt's ideas, there seems to be a relatively strong relationship between harmfulness and unexpectedness. 

```{r}
# Plot harmfulness by immorality

dsl %>% 
  ggplot(aes(x = mean_harm, y = mean_imm, col = type)) + 
  geom_point() + 
  geom_text_repel(aes(label = scenario), show.legend = F) + 
  theme(legend.position = "bottom") + 
  guides(text = F) + 
  labs(y = "Immorality", 
       x = "Harmfulness", 
       title = "Average Immorality by Average Harmfulness")
```

Consistent with our previous analysis, this relatioship is very strong, almost forming a 45 degree line. Perceptions of harmfulness are highly informative of respondents' conceptions of immorality. 

To close this visualization section, I am going to look at how deflection relates to our three measures. 

```{r}

# Unexpectedness by deflection 

dsl %>% 
  ggplot(aes(x = def, y = mean_unex, col = type)) + 
  geom_point() +
  geom_text_repel(aes(label = scenario), show.legend = F) + 
  theme(legend.position = "bottom") + 
  guides(text = F) + 
  labs(x = "Deflection", 
       y = "Unexpectedness", 
       title = "Average Unexpectedness by Deflection")


# Immorality by deflection 

dsl %>% 
  ggplot(aes(x = def, y = mean_imm, col = type)) + 
  geom_point() + 
  geom_text_repel(aes(label = scenario), show.legend = F) + 
  theme(legend.position = "bottom") + 
  guides(text = F) + 
  labs(x = "Deflection", 
       y = "Immorality", 
       title = "Average Immorality by Deflection")

# Harmfulness by deflection 

dsl %>% 
  ggplot(aes(x = def, y = mean_harm, col = type)) + 
  geom_point() + 
  geom_text_repel(aes(label = scenario), show.legend = F) + 
  theme(legend.position = "bottom") + 
  guides(text = F) + 
  labs(x = "Deflection", 
       y = "Harmfulness", 
       title = "Average Harmfulness by Deflection")

```

To an extent, our data show the results that are similar to what we expected. Deflection seems adequate to predict the values of events that obviously involve harm, but it really misses the mark when it comes trangressions that invole breaches of "purity". However, overall, deflection seems to have no relationship with either immorality or unexpectedness. The latter relationship is particularly troubling because "unexpectedness" maps on closely to the idea that the concept of deflection is meant to capture. 

## First Analysis 

I am going to try to fit multi-level models to see what best predicts immorality. For these initial exploratory models, I will be using nlme. I will let the intercepts for participants and scenarios vary, but here I won't explore more co-variances. 

I am going to scale all the variables first. 

```{r}
# Scale all variables before the analysis
d_long_scaled <- 
  d_long %>% 
  mutate_at(c("id", "type", "scenario", "conservative", "neg_beh"), ~as.factor(.)) %>% 
  mutate_if(is.numeric, scale)
```

Now, I am going to predict immorality on our main questions. In the most complex model, I will include an interaction effect between conservatism and the type of moral foundation. This accounts for the idea that political ideology mediates how much weight is placed on the different moral foundations. 

```{r}

# Fit initial model
m1 <- lme(fixed = immoral ~ harm + def + unexpected,
           data = d_long_scaled,
           random =  ~  1  | scenario/id)

summary(m1)

# Fit model controlling for conservatism 
m2 <- lme(fixed = immoral ~ harm + def + unexpected + conservative,
           data = d_long_scaled,
           random =  ~  1 | scenario/id)

summary(m2)

# Add moral foundations to the model and add an interaction with conservatism
m3 <- lme(fixed = immoral ~ harm + def + unexpected + conservative*type,
           data = d_long_scaled,
           random =  ~  1 | scenario/id)

summary(m3)
```

Here, the best fitting model is actually the most parsimonious one. Harm significantly predicts immorality and so does unexpectedness. Deflection, however, is a poor predictor of wrongdoing. One thing to take into account here is that there is not a lot of variation in deflection - we only have 25 data points - and, therefore, establishing relationships can be difficult. However, the sign and the magnitudes of the coefficients do not speak favorably of deflection's capacity to predict immorality. 

The third model is worth mentioning. Although the difference between conservatives and liberals is significant, it is not necessarily considerable. Interestingly, though, the interaction effects are also significant. The effect of "purity" on immorality, for instance, is bigger amongst conservatives than it is amongst liberals. Given the small overall differences, nonetheless, it is important not to overemphasize these mediating effects. However, it is a finding worth noting. 

Perhaps a more direct test of deflection's explanatory power in the moral domain would be to test its capacity to account for unexpectedness.

```{r}

# Unexpectedness exaplained by harm, immorality, and deflection

m4 <- lme( fixed = unexpected ~ 1 + def + harm + immoral,
           data = d_long_scaled,
           random =  ~  1 | scenario/id)

summary(m4)

```

This model suggests that, in our data, deflection is not very informative for predicting unexpectedness, unlike harm and immorality. The coefficient for deflection, here, is on the verge of significance but its sign is the opposite of what one would expect. The model suggests that as deflection goes up, unexpectedness goes down. Undoubtedly, this result is being influenced by the low deflection / high unexpectedness events such as incest. 

We can have a deeper look at whether the EPA profiles themselves tell us something about the immorality of acts. Similarly, we can examine whether transformations in these values - the individual components of deflection - are more informative than the aggregate value. I don't think we should put all these predictors in the same model due to multicollinearity - EPA profiles and subsequent tranformations carry much of the same information. I will then fit two models: one for the EPA profiles of actors, behaviors, and objects; the second, for the difference between those values and the transient impressions. 

```{r}

# Immorality predicted by the individual EPA profiles 

m5 <- lme(fixed = immoral ~ 1 + ae + ap + aa + be + bp + ba + oe + op + oa,
           data = d_long_scaled,
           random =  ~  1 | scenario/id)

summary(m5)

# Immorality predicted by the transformations in EPA profiles 

m6 <- lme(fixed = immoral ~ 1  + dae + dap + daa + dbe + dbp + dba + doe + dop + doa,
           data = d_long_scaled,
           random =  ~  1 | scenario/id)

summary(m6)
```

Both models seem to fit the data poorly (their BICs are quite high). Again, they suggests that these data do not allow us to make any solid inferences about the relationship between the EPA profiles associated with the elements of moral statements and their perceived immorality. The only value that comes close to the threshold of significance is the potency of the behavior. As behaviors become more potent, immorality appears to increase. However, we cannot draw strong conclusions from this. 

The second model is similar in its poor fit and in the high p-values of its coefficients. One coefficient, nonetheless, reaches significance: the change in potency of the object. It might appear counterintuitive but it's an artifact of the scale. All chosen events except for one - a person hires a prostitute - lower the potency of the object. However, the events that seem to lower the potency of the object the least are amongst the most immoral - such as a person hurting a child or a teacher hitting a lazy student. In this dataset, then, the events that lower the potency of the object the least are associated with more perceived immorality. Given that the scales are centered, this is what the coefficient is telling us. 

I will close this preliminary analysis by examining the predictive capacity of the EPA profiles - and the differences - when we exclude the purity trangressions. The rationale here is that ACT does a poor job predicting the deflection of these type of moral wrongdoings. If we exclude them, then, we could see whether this theory can pick up some generalities about the other set of trangressions. 


```{r}

# Run model with EPA scores on a sample excluding the purity scenarios 
m8 <- lme( fixed = immoral ~ 1 + ae + ap + aa + be + bp + ba + oe + op + oa,
           data = filter(d_long_scaled, type != "purity"),
           random =  ~  1 | scenario/id)

summary(m8)

m9 <- lme( fixed = immoral ~ 1 + dae + dap + daa + dbe + dbp + dba + doe + dop + doa,
           data = filter(d_long_scaled, type != "purity"),
           random =  ~  1 | scenario/id)

summary(m9)

```

The first thing to notice is that the BICs for these models is dramatically lower. We cannot compare them directly with the models fit above because the number of observations is not the same. However, it is worth bearing in mind that the EPA values seem to be a lot more informative when the purity scenarios are excluded. 

The first model shows interesting results. The coefficients for the behavior's evaluation and the object's potency are both significant. This resonates with Kurt's theory. As the evaluation of the behaviors go up, the immorality of a scenario sinificantly decreases. This is - reassuringly - what we would expect: behaviors that have low-evaluation are perceived as more immoral. The coefficient for the object's potency, in turn, suggests that as this value increases, the immorality of the act tends to decrease. I think this is an important finding because it speaks directly to Kurt's idea of vulnerability. As actors appear less potent, the behaviors directed at them are considered more immoral. This adds plausibility to the notion that the vulnerability of the victim is key when assessing immorality. 

## Conclusion

Let me recap some of the most important points here. 

1) We can show results similar to those presented by Gray and Keeney but we can also extend them. We have more data to show that unexpectedness is tied to immorality and that "purity" transgressions are still considered quite harmful. 

2) Deflection and EPA components are not predictive of immorality. However, when we exclude the purity scenarios from the analysis, we notice that EPAs are informative in predictable yet interesting ways. I think this works well with what we have been thinking. Deflections and EPA fail when it comes to purity scenarios because these events have "weird" structures: they involve good behaviors, which usually don't lower the potency of objects very much. If this line of argument is defensible - and methodologically sound - we have made good progress in explaining what's "weird" about harmless wrongs. 


